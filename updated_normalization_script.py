# -*- coding: utf-8 -*-
"""updated_normalization script.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1nwiNWdOtiV_LxoohbKFk5lrHhSU9ojU_
"""

#!pip install albumentations==0.4.6
#import albumentations 
#from albumentations.pytorch import ToTensorV2

from google.colab import drive
drive.mount('/content/gdrive')

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import os
from os import cpu_count
import torch
import torchvision
from torch.utils.data import Dataset, DataLoader

import albumentations as A
# albumentations.pytorch import ToTensorV2

import cv2

from tqdm import tqdm

import matplotlib.pyplot as plt
# %matplotlib inline


####### PARAMS

device      = torch.device('cpu') 
num_workers = 4
image_size  = 300, 400
batch_size  = 8
data_path   = '/content/gdrive/MyDrive/VOC2012'
#use import with google drive - it's faster

df = pd.read_csv(data_path + '/ImageSets/Segmentation/train.txt', header = None) #########################df.head()
print(df.iloc[1][0])

#from pyimagesearch import config

class ComputeData(Dataset):
    
    def __init__(self, 
                 data, 
                 directory, 
                 transform = None):
        self.data      = data
        self.directory = directory
        self.transform = transform
        
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        
        # import
        path  = os.path.join(self.directory, self.data.iloc[idx][0]+'.jpg') #['image_id']) #config.IMAGE_NAMES_PATH
        print(path)
        image = cv2.imread(path, cv2.COLOR_BGR2RGB)
        print(image.shape)
        # augmentations
        if self.transform is not None:
            image = self.transform(image)
        
        return image

from torchvision.transforms import CenterCrop
from torchvision.transforms import Compose
from torchvision.transforms import ToTensor
augs = Compose([ToTensor(),
                CenterCrop((300, 400))
                  #A.Normalize(mean = (0, 0, 0),
                  #            std  = (1, 1, 1)),
                  ])

image_dataset = ComputeData(data      = df, 
                         directory = data_path + '/JPEGImages/', 
                         transform = augs)

# data loader
image_loader = DataLoader(image_dataset, 
                          batch_size  = batch_size,
                          shuffle     = False, 
                          num_workers = cpu_count(),
                          #num_workers,
                          pin_memory  = True)

psum    = torch.tensor([0.0, 0.0, 0.0])
psum_sq = torch.tensor([0.0, 0.0, 0.0])

# loop through images
for inputs in tqdm(image_loader):
    
    psum    += inputs.sum(axis        = [0, 2, 3])
    psum_sq += (inputs ** 2).sum(axis = [0, 2, 3])

count = len(df) * image_size[0] * image_size[1]

# mean and std
total_mean = psum / count
total_var  = (psum_sq / count) - (total_mean ** 2)
total_std  = torch.sqrt(total_var)

# output
print('mean: '  + str(total_mean))
print('std:  '  + str(total_std))